{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIDE NOTE 1**\n",
    "\n",
    "Neural networks are about finding (or optimizing) a function that maps inputs to desired outputs. Essentially, they create a function that, for example, takes an image of a cat as input and outputs the label \"cat.\" This process involves learning patterns and relationships in the data to generalize and make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #The line import numpy as np is a Python statement that imports the numpy \n",
    "                 #library and assigns it the alias np. NumPy is a powerful library for numerical\n",
    "                 #computing in Python and is widely used in fields such as data science, machine \n",
    "                 #learning, and scientific computing. It provides support for working with large,\n",
    "                 #multi-dimensional arrays and matrices, along with a collection of mathematical \n",
    "                 #functions to operate on these data structures efficiently.\n",
    "import os  #The line import os is a Python statement that imports the os module,\n",
    "           #which is part of Python's standard library. The os module provides a\n",
    "           #wide range of functions for interacting with the operating system.\n",
    "           #It acts as a bridge between Python programs and the underlying operating \n",
    "           #system, allowing you to perform tasks such as file and directory manipulation, \n",
    "           #environment variable access, and process management.\n",
    "import urllib.request  #The line import urllib.request is a Python statement that imports the\n",
    "                       #request module from the urllib package. The urllib package is part of Python's \n",
    "                       #standard library and provides tools for working with URLs and handling HTTP requests. \n",
    "                       #By importing urllib.request, you gain access to a set of functions and classes that allow\n",
    "                       #you to interact with web resources, such as downloading files, fetching data from APIs, or \n",
    "                       #submitting HTTP requests.\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST dataset (mnist.npz) if not already available\n",
    "if not os.path.exists(\"mnist.npz\"):\n",
    "    print(\"Downloading mnist.npz...\")\n",
    "    url = \"https://s3.amazonaws.com/img-datasets/mnist.npz\"\n",
    "    urllib.request.urlretrieve(url, \"mnist.npz\")\n",
    "    # This code snippet checks if a file named `mnist.npz` exists in the current working directory. \n",
    "    # If the file does not exist, it downloads the file from a specified URL. Here's a breakdown of the logic:\n",
    "    # 1. **File Existence Check**: The condition `if not os.path.exists(\"mnist.npz\")` uses the `os.path.exists` \n",
    "    #    function to determine whether the file `mnist.npz` is present. If the file does not exist, the condition \n",
    "    #    evaluates to `True`, and the code inside the `if` block is executed.\n",
    "    # 2. **User Notification**: If the file is missing, the `print` function outputs the message `\"Downloading mnist.npz...\"` \n",
    "    #    to inform the user that the file is being downloaded.\n",
    "    # 3. **URL Definition**: The variable `url` is assigned the string `\"https://s3.amazonaws.com/img-datasets/mnist.npz\"`, \n",
    "    #    which is the web address where the file can be downloaded. This URL points to a hosted version of the MNIST dataset, \n",
    "    #    a popular dataset used for training and testing machine learning models.\n",
    "    # 4. **File Download**: The `urllib.request.urlretrieve` function is called with two arguments: the `url` and the local \n",
    "    #    filename `\"mnist.npz\"`. This function downloads the file from the specified URL and saves it to the current working \n",
    "    #    directory with the name `mnist.npz`.\n",
    "    # This code is a common pattern for ensuring that required resources, such as datasets, are available before proceeding \n",
    "    # with further computations. It avoids redundant downloads by checking for the file's existence first, which can save \n",
    "    # timeand  bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Load data from the .npz file\n",
    "data = np.load(\"mnist.npz\")\n",
    "x_train = data[\"x_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "x_test = data[\"x_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "# Preprocess data\n",
    "# Flatten the images and normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(np.float32) / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(np.float32) / 255.0\n",
    "#This code snippet preprocesses the MNIST dataset by flattening the images and normalizing their pixel values. \n",
    "#Preprocessing is an essential step in preparing data for machine learning models, \n",
    "#as it ensures the data is in a format suitable for training.\n",
    "# 1. **Flattening the Images**:  \n",
    "#    The `reshape(-1, 28 * 28)` operation transforms each image from its original 2D shape of `28x28` \n",
    "#    pixels into a 1D array of `784` pixels. \n",
    "#    The `-1` in the first dimension allows NumPy to automatically infer the number of samples based on \n",
    "#    the total size of the array. \n",
    "#    Flattening is necessary because many machine learning models, such as fully connected neural networks, \n",
    "#    input data to be in a 1D format rather than 2D.\n",
    "# 2. **Normalizing Pixel Values**:  \n",
    "#    The pixel values in the MNIST dataset typically range from `0` to `255`, \n",
    "#    representing grayscale intensity. \n",
    "#    Dividing by `255.0` scales these values to the range `[0, 1]`. Normalization is important because \n",
    "#    it ensures that all input features have a consistent scale, \n",
    "#    which can improve the convergence and stability of machine learning algorithms.\n",
    "# 3. **Data Type Conversion**:  \n",
    "#    The `astype(np.float32)` method converts the pixel values to the `float32` data type. \n",
    "#    This is necessary because many machine learning frameworks and models require input data to \n",
    "#    in a floating-point format for numerical computations.\n",
    "# 4. **Separate Processing for Training and Testing Data**:  \n",
    "#    The code applies the same preprocessing steps to both the training data (`x_train`) and the testing data (`x_test`). \n",
    "#    This ensures consistency between the datasets, which is crucial for evaluating the model's performance accurately.\n",
    "#    In summary, this preprocessing step prepares the MNIST images for input into a machine learning model by flattening \n",
    "#    them into 1D arrays, \n",
    "# normalizing their pixel values to a range of `[0, 1]`, and ensuring the data type is compatible with numerical computations. \n",
    "# These transformations are standard practices in image-based machine learning workflows.\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train_oh = one_hot(y_train, 10)\n",
    "y_test_oh = one_hot(y_test, 10)\n",
    "#Imagine you have a list of categories, like fruit types (apple, banana, orange).\n",
    "#One-hot encoding is a way to convert these categorical labels into a numerical format\n",
    "#that machine learning algorithms can understand. Instead of using a single number to represent each category,\n",
    "#we create a vector of zeros with a single '1' in the position corresponding to that category.\n",
    "#np.eye(num_classes): This creates an identity matrix. For num_classes=10, it generates a 10x10 matrix.\n",
    "#y_train_oh = one_hot(y_train, 10): This encodes the training labels (y_train) into a one-hot format.\n",
    "#y_test_oh = one_hot(y_test, 10): This encodes the testing labels (y_test) similarly.\n",
    "#The resulting y_train_oh and y_test_oh are 2D arrays where each row corresponds to the one-hot encoded \n",
    "#of a label.\n",
    "print(y_train_oh)\n",
    "print(y_test_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIDE NOTE 2**\n",
    "\n",
    "Gradient Descent Explained with f(x) = x² + 3\n",
    "\n",
    "Understanding the Function\n",
    "The function f(x) = x² + 3 represents a parabola (a U-shaped curve).  \n",
    "- The lowest point of this parabola is the minimum value of the function.  \n",
    "- In this simple equation, the minimum occurs at x = 0, where f(x) = 3.  \n",
    "- Gradient descent helps computers find the minimum of more complex equations.\n",
    "\n",
    "The Gradient (Slope)\n",
    "To find the slope, we calculate the derivative of the function:  \n",
    "- The derivative of f(x) = x² + 3 is f'(x) = 2x.  \n",
    "- This derivative, 2x, is the gradient. It tells us the slope of the curve at any point x.\n",
    "\n",
    "The Gradient Descent Process\n",
    "1. Start with a Guess  \n",
    "  - Let's start with x = 2.  \n",
    "  - At this point, f(2) = 2² + 3 = 7.\n",
    "\n",
    "2. Calculate the Gradient  \n",
    "  - The gradient at x = 2 is f'(2) = 2 * 2 = 4.  \n",
    "  - Since the slope is positive, the function is going uphill. To go downhill, we move in the opposite direction.\n",
    "\n",
    "3. Take a Step  \n",
    "  - Use the formula:  \n",
    "    x_new = x_old - (learning_rate * gradient)  \n",
    "  - With a learning rate of 0.1:  \n",
    "    x_new = 2 - (0.1 * 4) = 2 - 0.4 = 1.6.\n",
    "\n",
    "4. Repeat  \n",
    "  - Now, repeat the process with x = 1.6:  \n",
    "    - f'(1.6) = 2 * 1.6 = 3.2  \n",
    "    - x_new = 1.6 - (0.1 * 3.2) = 1.6 - 0.32 = 1.28  \n",
    "  - Each iteration brings x closer to 0, and f(x) closer to 3.\n",
    "\n",
    "Key Points\n",
    "- Learning Rate:  \n",
    "  - If the learning rate is too large, we might overshoot the minimum.  \n",
    "  - If it's too small, convergence will take a long time.  \n",
    "- Minimizing:  \n",
    "  - Gradient descent finds the x value that minimizes f(x).  \n",
    "  - In this case, the minimum is at x = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions and their derivatives\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "#ReLU formula is : f(x) = max(0,x)\n",
    "#If the function receives any negative input, it returns 0;\n",
    "#however, if the function receives any positive value x, it returns that value.\n",
    "#As a result, the output has a range of 0 to infinite.\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(np.float32)\n",
    " \n",
    "#The `relu_derivative` function computes the derivative of the ReLU function.\n",
    "#It returns `1` for positive input values and `0` for non-positive values. \n",
    "#This is implemented using the condition `(x > 0).astype(np.float32)`, where the boolean \n",
    "#result of the condition is cast to a floating-point number. \n",
    "#This derivative is crucial during backpropagation, as it determines how the weights of the network are updated.\n",
    "\n",
    "def softmax(x):\n",
    "    # subtract max for numerical stability\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "#The `softmax` function is used to convert a vector of raw scores (logits) into probabilities, \n",
    "#making it suitable for the output layer of classification models. \n",
    "#To ensure numerical stability, the function subtracts the maximum value in each row of the input `x` before \n",
    "#applying the exponential function. \n",
    "#This prevents potential overflow issues when dealing with large values. The result is normalized by dividing the exponentials \n",
    "#by their sum along the specified axis (`axis=1`), ensuring the output probabilities sum to 1 for each row.\n",
    "#This makes the `softmax` function ideal for multi-class classification tasks.\n",
    "#The axis=1 argument in functions like np.max and np.sum specifies that the operation should be performed along \n",
    "#the rows of a 2D array (i.e., across the columns). In the context of machine learning, \n",
    "#this is particularly useful when processing batch data, where each row represents a single sample,\n",
    "#and each column corresponds to a feature or class score.\n",
    "#The axis=0 argument specifies that the operation should be applied column-wise,\n",
    "#meaning it processes each column independently.\n",
    "#The keepdims=True argument ensures that the resulting arrays maintain their original dimensions,\n",
    "#which is necessary for broadcasting during division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 28 * 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "#The active selection defines a variable, `batch_size`, and assigns it the value `128`. \n",
    "#In the context of machine learning or deep learning, \n",
    "#the `batch_size` determines the number of samples processed together in a single forward and backward pass during training.\n",
    "#When training a model, the dataset is often too large to fit into memory all at once. Instead, \n",
    "#the data is divided into smaller subsets called batches. \n",
    "#The `batch_size` specifies the size of each of these subsets. For example, \n",
    "#if the dataset contains 10,000 samples and the `batch_size` is set to `128`, \n",
    "#the model will process the data in chunks of 128 samples at a time, requiring approximately \n",
    "#78 iterations (10,000 ÷ 128) to complete one epoch (a full pass through the dataset).\n",
    "#Choosing an appropriate `batch_size` is important for balancing computational efficiency and model performance.\n",
    "#Smaller batch sizes can lead to more frequent updates to the model's parameters, '\n",
    "#'potentially improving convergence but at the cost of increased computational overhead. Larger batch sizes,\n",
    "#like `128` in this case, are more memory-efficient and can take advantage of parallel '\n",
    "#'processing on GPUs, but they may require more epochs to achieve optimal performance. The value `128` \n",
    "#is a common choice as it strikes a balance between these trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases with He initialization for ReLU layers\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# The variable W1 represents the weight matrix connecting the input layer to the hidden layer.\n",
    "# It is initialized using random values drawn from a normal distribution (np.random.randn) and \n",
    "# by the factor np.sqrt(2.0 / input_size). This scaling is based on the He initialization method, \n",
    "# which helps maintain stable gradients during training by accounting for the size of the input layer. \n",
    "# Without proper initialization, gradients can either vanish or explode, making training inefficient or unstable.\n",
    "# The variable b1 is the bias vector for the hidden layer. It is initialized to zeros using np.zeros.\n",
    "# Biases allow the model to shift activation functions, enabling it to better fit the data.\n",
    "# Similarly, W2 is the weight matrix connecting the hidden layer to the output layer.\n",
    "# It is also initialized with random values scaled by np.sqrt(2.0 / hidden_size) to ensure stable gradients. \n",
    "# The variable b2 is the bias vector for the output layer, initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_samples = x_train.shape[0]\n",
    "num_batches = num_samples // batch_size\n",
    "\n",
    "# x_train.shape returns a tuple representing the dimensions of the x_train array. For example, \n",
    "# if x_train has 60,000 samples and 784 features (e.g., for MNIST), x_train.shape would return (60000, 784).\n",
    "# [0] accesses the first element of this tuple, which corresponds to the number of samples (60,000 in this case).\n",
    "# db2 = np.sum(dz2, axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4669\n",
      "Epoch 2/10, Loss: 0.2588\n",
      "Epoch 3/10, Loss: 0.2083\n",
      "Epoch 4/10, Loss: 0.1770\n",
      "Epoch 5/10, Loss: 0.1538\n",
      "Epoch 6/10, Loss: 0.1368\n",
      "Epoch 7/10, Loss: 0.1229\n",
      "Epoch 8/10, Loss: 0.1118\n",
      "Epoch 9/10, Loss: 0.1030\n",
      "Epoch 10/10, Loss: 0.0952\n",
      "Test accuracy: 96.97%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Shuffle the training data at the start of each epoch\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    y_train_oh = y_train_oh[indices]\n",
    "\n",
    "#The variable indices is created using np.arange(num_samples), which generates a sequence of integers from 0 to num_samples. \n",
    "#These indices correspond to the positions of the training samples in the dataset. The np.random.\n",
    "#(indices) function then randomly rearranges these indices, effectively shuffling the order of the dataset.\n",
    "#The shuffled indices are used to reorder both the input data (x_train) and the corresponding one-hot encoded labels (y_train_oh). \n",
    "#ensures that the training data is presented to the model in a different order during each epoch,\n",
    "#which helps the model learn more robust patterns and reduces the risk of bias caused by the original order of the data.\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        x_batch = x_train[start:end]\n",
    "        y_batch = y_train_oh[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        z1 = np.dot(x_batch, W1) + b1          # Linear transformation for hidden layer\n",
    "        a1 = relu(z1)                          # ReLU activation\n",
    "        z2 = np.dot(a1, W2) + b2               # Linear transformation for output layer\n",
    "        a2 = softmax(z2)                       # Softmax activation for probabilities\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = -np.sum(y_batch * np.log(a2 + 1e-8)) / batch_size\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward pass (gradient computation)\n",
    "        dz2 = a2 - y_batch                     # Derivative of loss w.r.t. z2\n",
    "        dW2 = np.dot(a1.T, dz2) / batch_size\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        da1 = np.dot(dz2, W2.T)\n",
    "        dz1 = da1 * relu_derivative(z1)        # Backprop through ReLU\n",
    "        dW1 = np.dot(x_batch.T, dz1) / batch_size\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation on test set\n",
    "z1_test = np.dot(x_test, W1) + b1\n",
    "a1_test = relu(z1_test)\n",
    "z2_test = np.dot(a1_test, W2) + b2\n",
    "a2_test = softmax(z2_test)\n",
    "predictions = np.argmax(a2_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
