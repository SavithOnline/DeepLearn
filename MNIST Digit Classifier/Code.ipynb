{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are about finding (or optimizing) a function that maps inputs to desired outputs. Essentially, they create a function that, for example, takes an image of a cat as input and outputs the label \"cat.\" This process involves learning patterns and relationships in the data to generalize and make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #The line import numpy as np is a Python statement that imports the numpy \n",
    "                 #library and assigns it the alias np. NumPy is a powerful library for numerical\n",
    "                 #computing in Python and is widely used in fields such as data science, machine \n",
    "                 #learning, and scientific computing. It provides support for working with large,\n",
    "                 #multi-dimensional arrays and matrices, along with a collection of mathematical \n",
    "                 #functions to operate on these data structures efficiently.\n",
    "import os  #The line import os is a Python statement that imports the os module,\n",
    "           #which is part of Python's standard library. The os module provides a\n",
    "           #wide range of functions for interacting with the operating system.\n",
    "           #It acts as a bridge between Python programs and the underlying operating \n",
    "           #system, allowing you to perform tasks such as file and directory manipulation, \n",
    "           #environment variable access, and process management.\n",
    "import urllib.request  #The line import urllib.request is a Python statement that imports the\n",
    "                       #request module from the urllib package. The urllib package is part of Python's \n",
    "                       #standard library and provides tools for working with URLs and handling HTTP requests. \n",
    "                       #By importing urllib.request, you gain access to a set of functions and classes that allow\n",
    "                       #you to interact with web resources, such as downloading files, fetching data from APIs, or \n",
    "                       #submitting HTTP requests.\n",
    "import npzviewer # Open and inspect the mnist.npz file using npzviewer\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST dataset (mnist.npz) if not already available\n",
    "if not os.path.exists(\"mnist.npz\"):\n",
    "    print(\"Downloading mnist.npz...\")\n",
    "    url = \"https://s3.amazonaws.com/img-datasets/mnist.npz\"\n",
    "    urllib.request.urlretrieve(url, \"mnist.npz\")\n",
    "    # This code snippet checks if a file named `mnist.npz` exists in the current working directory. \n",
    "    # If the file does not exist, it downloads the file from a specified URL. Here's a breakdown of the logic:\n",
    "    # 1. **File Existence Check**: The condition `if not os.path.exists(\"mnist.npz\")` uses the `os.path.exists` \n",
    "    #    function to determine whether the file `mnist.npz` is present. If the file does not exist, the condition \n",
    "    #    evaluates to `True`, and the code inside the `if` block is executed.\n",
    "    # 2. **User Notification**: If the file is missing, the `print` function outputs the message `\"Downloading mnist.npz...\"` \n",
    "    #    to inform the user that the file is being downloaded.\n",
    "    # 3. **URL Definition**: The variable `url` is assigned the string `\"https://s3.amazonaws.com/img-datasets/mnist.npz\"`, \n",
    "    #    which is the web address where the file can be downloaded. This URL points to a hosted version of the MNIST dataset, \n",
    "    #    a popular dataset used for training and testing machine learning models.\n",
    "    # 4. **File Download**: The `urllib.request.urlretrieve` function is called with two arguments: the `url` and the local \n",
    "    #    filename `\"mnist.npz\"`. This function downloads the file from the specified URL and saves it to the current working \n",
    "    #    directory with the name `mnist.npz`.\n",
    "    # This code is a common pattern for ensuring that required resources, such as datasets, are available before proceeding \n",
    "    # with further computations. It avoids redundant downloads by checking for the file's existence first, which can save \n",
    "    # timeand  bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the .npz file: ['x_test', 'x_train', 'y_train', 'y_test']\n",
      "x_test: shape (10000, 28, 28), dtype uint8\n",
      "x_train: shape (60000, 28, 28), dtype uint8\n",
      "y_train: shape (60000,), dtype uint8\n",
      "y_test: shape (10000,), dtype uint8\n"
     ]
    }
   ],
   "source": [
    "# Load data from the .npz file\n",
    "data = np.load(\"mnist.npz\")\n",
    "x_train = data[\"x_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "x_test = data[\"x_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "# Inspect the contents of the mnist.npz file\n",
    "print(\"Keys in the .npz file:\", data.files)\n",
    "for key in data.files:\n",
    "    print(f\"{key}: shape {data[key].shape}, dtype {data[key].dtype}\")\n",
    "\n",
    "# Preprocess data\n",
    "# Flatten the images and normalize pixel values to [0, 1]\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(np.float32) / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(np.float32) / 255.0\n",
    "# This code snippet preprocesses the MNIST dataset by flattening the images and normalizing their pixel values. \n",
    "# Preprocessing is an essential step in preparing data for machine learning models, as it ensures the data is in a format suitable for training.\n",
    "# 1. **Flattening the Images**:  \n",
    "#    The `reshape(-1, 28 * 28)` operation transforms each image from its original 2D shape of `28x28` pixels into a 1D array of `784` pixels. \n",
    "#    The `-1` in the first dimension allows NumPy to automatically infer the number of samples based on the total size of the array. \n",
    "#    Flattening is necessary because many machine learning models, such as fully connected neural networks, expect input data to be in a 1D format rather than 2D.\n",
    "# 2. **Normalizing Pixel Values**:  \n",
    "#    The pixel values in the MNIST dataset typically range from `0` to `255`, representing grayscale intensity. \n",
    "#    Dividing by `255.0` scales these values to the range `[0, 1]`. Normalization is important because it ensures that all input features have a consistent scale, \n",
    "#    which can improve the convergence and stability of machine learning algorithms.\n",
    "# 3. **Data Type Conversion**:  \n",
    "#    The `astype(np.float32)` method converts the pixel values to the `float32` data type. \n",
    "#    This is necessary because many machine learning frameworks and models require input data to be in a floating-point format for numerical computations.\n",
    "# 4. **Separate Processing for Training and Testing Data**:  \n",
    "#    The code applies the same preprocessing steps to both the training data (`x_train`) and the testing data (`x_test`). \n",
    "#    This ensures consistency between the datasets, which is crucial for evaluating the model's performance accurately.\n",
    "# In summary, this preprocessing step prepares the MNIST images for input into a machine learning model by flattening them into 1D arrays, \n",
    "# normalizing their pixel values to a range of `[0, 1]`, and ensuring the data type is compatible with numerical computations. \n",
    "# These transformations are standard practices in image-based machine learning workflows.\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train_oh = one_hot(y_train, 10)\n",
    "y_test_oh = one_hot(y_test, 10)\n",
    "# Imagine you have a list of categories, like fruit types (apple, banana, orange).\n",
    "# One-hot encoding is a way to convert these categorical labels into a numerical format\n",
    "# that machine learning algorithms can understand. Instead of using a single number to represent each category,\n",
    "# we create a vector of zeros with a single '1' in the position corresponding to that category.\n",
    "# np.eye(num_classes): This creates an identity matrix. For num_classes=10, it generates a 10x10 matrix.\n",
    "# y_train_oh = one_hot(y_train, 10): This encodes the training labels (y_train) into a one-hot format.\n",
    "# y_test_oh = one_hot(y_test, 10): This encodes the testing labels (y_test) similarly.\n",
    "# The resulting y_train_oh and y_test_oh are 2D arrays where each row corresponds to the one-hot encoded representation of a label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is unrelated to neural networks and is included to provide a better understanding of one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_explanation(y, num_classes=5):\n",
    "    # Create the identity matrix\n",
    "    identity = np.eye(num_classes)\n",
    "    \n",
    "    # Select rows based on labels\n",
    "    one_hot_encoded = identity[y]\n",
    "    \n",
    "    return one_hot_encoded\n",
    "\n",
    "# Example\n",
    "labels = [2, 3, 1]\n",
    "result = one_hot_explanation(labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Explained with f(x) = x² + 3\n",
    "\n",
    "#### Understanding the Function\n",
    "The function `f(x) = x² + 3` represents a parabola (a U-shaped curve).  \n",
    "- The lowest point of this parabola is the minimum value of the function.  \n",
    "- In this simple equation, the minimum occurs at `x = 0`, where `f(x) = 3`.  \n",
    "- Gradient descent helps computers find the minimum of more complex equations.\n",
    "\n",
    "#### The Gradient (Slope)\n",
    "To find the slope, we calculate the derivative of the function:  \n",
    "- The derivative of `f(x) = x² + 3` is `f'(x) = 2x`.  \n",
    "- This derivative, `2x`, is the gradient. It tells us the slope of the curve at any point `x`.\n",
    "\n",
    "#### The Gradient Descent Process\n",
    "1. **Start with a Guess**  \n",
    "    - Let's start with `x = 2`.  \n",
    "    - At this point, `f(2) = 2² + 3 = 7`.\n",
    "\n",
    "2. **Calculate the Gradient**  \n",
    "    - The gradient at `x = 2` is `f'(2) = 2 * 2 = 4`.  \n",
    "    - Since the slope is positive, the function is going uphill. To go downhill, we move in the opposite direction.\n",
    "\n",
    "3. **Take a Step**  \n",
    "    - Use the formula:  \n",
    "      `x_new = x_old - (learning_rate * gradient)`  \n",
    "    - With a learning rate of `0.1`:  \n",
    "      `x_new = 2 - (0.1 * 4) = 2 - 0.4 = 1.6`.\n",
    "\n",
    "4. **Repeat**  \n",
    "    - Now, repeat the process with `x = 1.6`:  \n",
    "      - `f'(1.6) = 2 * 1.6 = 3.2`  \n",
    "      - `x_new = 1.6 - (0.1 * 3.2) = 1.6 - 0.32 = 1.28`  \n",
    "    - Each iteration brings `x` closer to `0`, and `f(x)` closer to `3`.\n",
    "\n",
    "#### Key Points\n",
    "- **Learning Rate**:  \n",
    "  - If the learning rate is too large, we might overshoot the minimum.  \n",
    "  - If it's too small, convergence will take a long time.  \n",
    "- **Minimizing**:  \n",
    "  - Gradient descent finds the `x` value that minimizes `f(x)`.  \n",
    "  - In this case, the minimum is at `x = 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4511\n",
      "Epoch 2/10, Loss: 0.2528\n",
      "Epoch 3/10, Loss: 0.2043\n",
      "Epoch 4/10, Loss: 0.1734\n",
      "Epoch 5/10, Loss: 0.1511\n",
      "Epoch 6/10, Loss: 0.1339\n",
      "Epoch 7/10, Loss: 0.1214\n",
      "Epoch 8/10, Loss: 0.1104\n",
      "Epoch 9/10, Loss: 0.1014\n",
      "Epoch 10/10, Loss: 0.0936\n",
      "Test accuracy: 96.89%\n"
     ]
    }
   ],
   "source": [
    "# Activation functions and their derivatives\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(np.float32)\n",
    "\n",
    "def softmax(x):\n",
    "    # subtract max for numerical stability\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Initialize weights and biases with He initialization for ReLU layers\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Training loop\n",
    "num_samples = x_train.shape[0]\n",
    "num_batches = num_samples // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the training data at the start of each epoch\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    y_train_oh = y_train_oh[indices]\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        x_batch = x_train[start:end]\n",
    "        y_batch = y_train_oh[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        z1 = np.dot(x_batch, W1) + b1          # Linear transformation for hidden layer\n",
    "        a1 = relu(z1)                          # ReLU activation\n",
    "        z2 = np.dot(a1, W2) + b2               # Linear transformation for output layer\n",
    "        a2 = softmax(z2)                       # Softmax activation for probabilities\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = -np.sum(y_batch * np.log(a2 + 1e-8)) / batch_size\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Backward pass (gradient computation)\n",
    "        dz2 = a2 - y_batch                     # Derivative of loss w.r.t. z2\n",
    "        dW2 = np.dot(a1.T, dz2) / batch_size\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        da1 = np.dot(dz2, W2.T)\n",
    "        dz1 = da1 * relu_derivative(z1)        # Backprop through ReLU\n",
    "        dW1 = np.dot(x_batch.T, dz1) / batch_size\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation on test set\n",
    "z1_test = np.dot(x_test, W1) + b1\n",
    "a1_test = relu(z1_test)\n",
    "z2_test = np.dot(a1_test, W2) + b2\n",
    "a2_test = softmax(z2_test)\n",
    "predictions = np.argmax(a2_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
