{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset using TensorFlow...\n",
      "Data loaded. Training set: (60000, 28, 28), Test set: (10000, 28, 28)\n",
      "Training CNN...\n",
      "Starting training for 1 epochs...\n",
      "  Epoch 1, Iteration 100/600, Batch Loss: 0.6225\n",
      "  Epoch 1, Iteration 200/600, Batch Loss: 0.5307\n",
      "  Epoch 1, Iteration 300/600, Batch Loss: 0.2840\n",
      "  Epoch 1, Iteration 400/600, Batch Loss: 0.3330\n",
      "  Epoch 1, Iteration 500/600, Batch Loss: 0.3331\n",
      "  Epoch 1, Iteration 600/600, Batch Loss: 0.2047\n",
      "Epoch 1/1 finished. Evaluating...\n",
      "  Avg Loss: 0.4851, Train Acc (on 1000): 0.9290, Val Acc (on 1000): 0.9290\n",
      "------------------------------\n",
      "Evaluating on full test set...\n",
      "Test accuracy: 0.9388\n",
      "Model parameters saved to mnist_cnn_params.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "# Suppress TensorFlow logs if desired (optional)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "# from scipy.special import softmax # Using manual implementation now\n",
    "\n",
    "# Use manual softmax for stability and consistency\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(1, -1)\n",
    "    # Subtract max for numerical stability\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def load_mnist():\n",
    "    \"\"\"\n",
    "    Load MNIST dataset using TensorFlow's datasets module\n",
    "    \"\"\"\n",
    "    print(\"Loading MNIST dataset using TensorFlow...\")\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Normalize pixel values to [0, 1] and ensure float32\n",
    "    X_train = x_train.astype(np.float32) / 255.0\n",
    "    X_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "    y_train = np.array(y_train, dtype=np.int32) # Use int32 for labels\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "    print(f\"Data loaded. Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Transform images to columns for efficient convolution operation.\n",
    "    Ensures float32 output.\n",
    "    \"\"\"\n",
    "    # Ensure input is float32\n",
    "    input_data = input_data.astype(np.float32, copy=False)\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
    "    # Initialize output with float32\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w), dtype=np.float32)\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Transform columns back to image format.\n",
    "    Ensures float32 output.\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    # Initialize output with float32\n",
    "    img = np.zeros((N, C, H + 2*pad, W + 2*pad), dtype=np.float32)\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    if pad > 0:\n",
    "        return img[:, :, pad:-pad, pad:-pad]\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, input_channels, output_channels, kernel_size=3, stride=1, pad=1, learning_rate=0.01):\n",
    "        scale = np.sqrt(1.0 / (input_channels * kernel_size * kernel_size)).astype(np.float32)\n",
    "        self.W = scale * np.random.randn(output_channels, input_channels, kernel_size, kernel_size).astype(np.float32)\n",
    "        self.b = np.zeros(output_channels, dtype=np.float32)\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.lr = learning_rate\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = (H + 2*self.pad - FH) // self.stride + 1\n",
    "        out_w = (W + 2*self.pad - FW) // self.stride + 1\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        # N, _, out_h, out_w = dout.shape # Get N from dout if needed, or rely on self.x\n",
    "        dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "        self.db = np.sum(dout_reshaped, axis=0)\n",
    "        dW_col = np.dot(self.col.T, dout_reshaped)\n",
    "        self.dW = dW_col.T.reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout_reshaped, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class MaxPooling:\n",
    "    def __init__(self, pool_size=2, stride=2, pad=0):\n",
    "        self.pool_h = pool_size\n",
    "        self.pool_w = pool_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = (H + 2*self.pad - self.pool_h) // self.stride + 1\n",
    "        out_w = (W + 2*self.pad - self.pool_w) // self.stride + 1\n",
    "        x_reshaped = x.reshape(N*C, 1, H, W)\n",
    "        col = im2col(x_reshaped, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        # col shape: (N*C*out_h*out_w, pool_h*pool_w)\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, C, out_h, out_w)\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, C, out_h, out_w = dout.shape\n",
    "        N_x, C_x, H_x, W_x = self.x.shape\n",
    "        dout_flat = dout.flatten()\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax_size = N * C * out_h * out_w # Or self.arg_max.size\n",
    "        # Initialize with float32\n",
    "        dmax = np.zeros((dmax_size, pool_size), dtype=np.float32)\n",
    "        dmax[np.arange(dmax_size), self.arg_max] = dout_flat\n",
    "        dcol = dmax\n",
    "        dx_reshaped_shape = (N*C, 1, H_x, W_x)\n",
    "        dx_reshaped = col2im(dcol, dx_reshaped_shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        dx = dx_reshaped.reshape(self.x.shape)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.original_shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_shape = x.shape\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape(batch_size, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.original_shape)\n",
    "\n",
    "\n",
    "class FullyConnected:\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
    "        # He initialization might be better with ReLU, Glorot here\n",
    "        scale = np.sqrt(2.0 / (input_size + output_size)).astype(np.float32)\n",
    "        self.W = scale * np.random.randn(input_size, output_size).astype(np.float32)\n",
    "        self.b = np.zeros(output_size, dtype=np.float32)\n",
    "        self.x = None\n",
    "        self.lr = learning_rate\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x.astype(np.float32, copy=False) # Ensure float32\n",
    "        return np.dot(self.x, self.W) + self.b\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.astype(np.float32, copy=True) # Work on a float32 copy\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        return dout\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # Output probabilities\n",
    "        self.t = None # Target labels (one-hot)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Ensure input is float32\n",
    "        x = x.astype(np.float32, copy=False)\n",
    "        self.t = t # Store original t for potential use\n",
    "\n",
    "        # Handle integer labels -> convert to one-hot\n",
    "        if t.ndim == 1 or t.shape[1] == 1:\n",
    "             num_classes = x.shape[1]\n",
    "             t_flat = t.flatten().astype(np.int32) # Ensure integer indices\n",
    "             t_one_hot = np.zeros((t_flat.size, num_classes), dtype=np.float32)\n",
    "             t_one_hot[np.arange(t_flat.size), t_flat] = 1.0\n",
    "             self.t_one_hot = t_one_hot # Store one-hot version\n",
    "        else:\n",
    "             self.t_one_hot = t.astype(np.float32, copy=False) # Assume already one-hot\n",
    "\n",
    "        # Calculate stable softmax\n",
    "        self.y = softmax(x) # Use the stable softmax function defined outside\n",
    "\n",
    "        # Calculate cross-entropy loss\n",
    "        epsilon = 1e-7 # Small value to prevent log(0)\n",
    "        batch_size = self.t_one_hot.shape[0]\n",
    "        self.loss = -np.sum(self.t_one_hot * np.log(self.y + epsilon)) / batch_size\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t_one_hot.shape[0]\n",
    "        # Gradient of loss w.r.t softmax input\n",
    "        dx = (self.y - self.t_one_hot) / batch_size\n",
    "        dx = dx * dout # Apply upstream gradient scaling\n",
    "        return dx\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        input_channels = 1\n",
    "        conv1_filters = 32\n",
    "        conv2_filters = 64\n",
    "        flattened_size = conv2_filters * 7 * 7\n",
    "        fc1_units = 128\n",
    "        fc2_units = 10\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self.conv1 = Convolution(input_channels, conv1_filters, 3, 1, 1, self.lr)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPooling(2, 2)\n",
    "        self.conv2 = Convolution(conv1_filters, conv2_filters, 3, 1, 1, self.lr)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPooling(2, 2)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = FullyConnected(flattened_size, fc1_units, self.lr)\n",
    "        self.relu3 = ReLU()\n",
    "        self.fc2 = FullyConnected(fc1_units, fc2_units, self.lr)\n",
    "        self.softmax = SoftmaxWithLoss()\n",
    "\n",
    "        self.layers = [\n",
    "            self.conv1, self.relu1, self.pool1,\n",
    "            self.conv2, self.relu2, self.pool2,\n",
    "            self.flatten, self.fc1, self.relu3, self.fc2\n",
    "        ]\n",
    "        self.params = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Ensure input has 4 dimensions (N, C, H, W) and float32\n",
    "        if x.ndim == 3:\n",
    "            x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
    "        elif x.ndim == 2:\n",
    "             x = x.reshape(1, 1, x.shape[0], x.shape[1])\n",
    "        x = x.astype(np.float32, copy=False) # Ensure float32\n",
    "\n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "             h = layer.forward(h)\n",
    "        return h\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.softmax.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100): # Added batch_size\n",
    "        n_data = x.shape[0]\n",
    "        acc = 0.0\n",
    "\n",
    "        # Ensure x is correctly shaped N, C, H, W\n",
    "        if x.ndim == 3:\n",
    "             x = x.reshape(n_data, 1, x.shape[1], x.shape[2])\n",
    "        x = x.astype(np.float32, copy=False) # Ensure float32\n",
    "\n",
    "        # Ensure targets t are class indices (N,)\n",
    "        if t.ndim == 2:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        t = t.astype(np.int32) # Ensure int indices\n",
    "\n",
    "        for i in range(0, n_data, batch_size):\n",
    "            x_batch = x[i : i + batch_size]\n",
    "            t_batch = t[i : i + batch_size]\n",
    "            y_batch = self.predict(x_batch) # Predict handles internal float32 conversion\n",
    "            y_pred = np.argmax(y_batch, axis=1)\n",
    "            acc += np.sum(y_pred == t_batch)\n",
    "\n",
    "        return acc / n_data\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # Ensure input is 4D float32 for forward pass\n",
    "        if x.ndim == 3:\n",
    "            x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
    "        x = x.astype(np.float32, copy=False)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        self.loss(x, t)\n",
    "        # 2. Backward pass\n",
    "        dout = 1.0 # Start with float gradient\n",
    "        dout = self.softmax.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        # Gradients are now stored in self.params layers\n",
    "\n",
    "    def update_params(self):\n",
    "        for param_layer in self.params:\n",
    "             # Ensure gradients are float32 (should be if inputs were)\n",
    "             dW = param_layer.dW.astype(np.float32, copy=False)\n",
    "             db = param_layer.db.astype(np.float32, copy=False)\n",
    "             param_layer.W -= self.lr * dW\n",
    "             param_layer.b -= self.lr * db\n",
    "\n",
    "    def train(self, x_train, t_train, x_val, t_val, epochs=5, batch_size=100):\n",
    "        train_size = x_train.shape[0]\n",
    "        iter_per_epoch = max(train_size // batch_size, 1)\n",
    "        train_loss_list, train_acc_list, val_acc_list = [], [], []\n",
    "\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        for epoch in range(epochs):\n",
    "            idx = np.random.permutation(train_size)\n",
    "            x_train_shuffled = x_train[idx]\n",
    "            t_train_shuffled = t_train[idx]\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for i in range(iter_per_epoch):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                x_batch = x_train_shuffled[start:end] # Shape (N, H, W)\n",
    "                t_batch = t_train_shuffled[start:end] # Shape (N,)\n",
    "                # gradient method handles reshape and dtype\n",
    "                self.gradient(x_batch, t_batch)\n",
    "                self.update_params()\n",
    "                loss = self.softmax.loss\n",
    "                train_loss_list.append(loss)\n",
    "                epoch_loss += loss\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(f\"  Epoch {epoch + 1}, Iteration {i + 1}/{iter_per_epoch}, Batch Loss: {loss:.4f}\")\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} finished. Evaluating...\")\n",
    "            eval_batch_size = 500\n",
    "            eval_size = 1000 # Evaluate on a subset\n",
    "            train_acc = self.accuracy(x_train[:eval_size], t_train[:eval_size], batch_size=eval_batch_size)\n",
    "            val_acc = self.accuracy(x_val[:eval_size], t_val[:eval_size], batch_size=eval_batch_size)\n",
    "            train_acc_list.append(train_acc)\n",
    "            val_acc_list.append(val_acc)\n",
    "            avg_epoch_loss = epoch_loss / iter_per_epoch\n",
    "\n",
    "            print(f\"  Avg Loss: {avg_epoch_loss:.4f}, Train Acc (on {eval_size}): {train_acc:.4f}, Val Acc (on {eval_size}): {val_acc:.4f}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "        return train_loss_list, train_acc_list, val_acc_list\n",
    "\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        params_to_save = {\n",
    "            'conv1_W': self.conv1.W, 'conv1_b': self.conv1.b,\n",
    "            'conv2_W': self.conv2.W, 'conv2_b': self.conv2.b,\n",
    "            'fc1_W': self.fc1.W, 'fc1_b': self.fc1.b,\n",
    "            'fc2_W': self.fc2.W, 'fc2_b': self.fc2.b,\n",
    "            'lr': self.lr\n",
    "        }\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(params_to_save, f)\n",
    "        print(f\"Model parameters saved to {file_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_path, learning_rate=0.01):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            params_loaded = pickle.load(f)\n",
    "        lr = params_loaded.get('lr', learning_rate)\n",
    "        model = CNN(learning_rate=lr)\n",
    "        model.conv1.W = params_loaded['conv1_W']\n",
    "        model.conv1.b = params_loaded['conv1_b']\n",
    "        model.conv2.W = params_loaded['conv2_W']\n",
    "        model.conv2.b = params_loaded['conv2_b']\n",
    "        model.fc1.W = params_loaded['fc1_W']\n",
    "        model.fc1.b = params_loaded['fc1_b']\n",
    "        model.fc2.W = params_loaded['fc2_W']\n",
    "        model.fc2.b = params_loaded['fc2_b']\n",
    "        print(f\"Model parameters loaded from {file_path}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train, X_test, y_test = load_mnist()\n",
    "    model = CNN(learning_rate=0.01)\n",
    "    print(\"Training CNN...\")\n",
    "    # Train for a few epochs if desired\n",
    "    train_loss, train_acc, val_acc = model.train(X_train, y_train, X_test, y_test, epochs=1, batch_size=100)\n",
    "\n",
    "    print(\"Evaluating on full test set...\")\n",
    "    # accuracy method now handles reshape and batching\n",
    "    test_acc = model.accuracy(X_test, y_test, batch_size=500) # Use batching\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    model.save_model('mnist_cnn_params.pkl')\n",
    "\n",
    "    # Optional: Load and test again\n",
    "    # print(\"\\nLoading model and re-evaluating...\")\n",
    "    # loaded_model = CNN.load_model('mnist_cnn_params.pkl')\n",
    "    # # Ensure X_test is passed directly, accuracy handles reshape\n",
    "    # test_acc_loaded = loaded_model.accuracy(X_test, y_test, batch_size=500)\n",
    "    # print(f\"Loaded Model Test accuracy: {test_acc_loaded:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
